import logging

QA_LANGUAGES = [
    "ar", "bn", "en", "fi", "id", 'ko', 'ru', 'sw', 'te'
]

NLLB_CODES = {
    "ar": "arb_Arab",
    "bn": "ben_Beng",
    "en": "eng_Latn",
    "fi": "fin_Latn",
    "id": "ind_Latn",
    'ko': "kor_Hang",
    'ru': "rus_Cyrl",
    'sw': "swh_Latn",
    'te': "tel_Telu",
}

localrules: data_for_qa_finetuning

rule all:
    input:
        #expand("qa_experiments/baseline/{lang}/{split}.score",
        #       lang=QA_LANGUAGES, split=["validation", "test"]),
        #expand("qa_data/{lang}/train.{type}.eng",
        #       lang=QA_LANGUAGES, type=["question", "context"]),
        #"qa_experiments/baseline/en/tiny.answers",
        #expand("qa_data/{lang}/{split}.answers.eng",
        #       lang=QA_LANGUAGES, split=["train", "validation", "test"])
        #"qa_experiments/en/finetuning/success"
        "qa_experiments/finetune/en/test.answers"


rule download_xtreme_up:
    output:
        expand("xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl",
               split=['train', 'dev', 'test'],
               lang=QA_LANGUAGES)
    shell:
        """
        wget https://storage.googleapis.com/xtreme-up/xtreme-up-v1.1.jsonl.tgz
        tar -xzf xtreme-up-v1.1.jsonl.tgz
        rm xtreme-up-v1.1.jsonl.tgz
        """


rule format_xtreme_up_qa:
    input:
        "xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl"
    output:
        question="qa_data/{lang}/{split}.question.txt",
        context="qa_data/{lang}/{split}.context.txt",
        anwser="qa_data/{lang}/{split}.answers",
    run:
        import json
        with open(input[0], encoding="utf-8") as f:
            data = [json.loads(line) for line in f]
        with open(output.question, "w", encoding="utf-8") as f_question, \
             open(output.context, "w", encoding="utf-8") as f_context, \
             open(output.anwser, "w", encoding="utf-8") as f_answer:
            for item in data:
                print(item['question'], file=f_question)
                print(f"{item['title']}. {item['context']}", file=f_context)
                print(item['target'], file=f_answer)


rule split_contexts_to_sentences:
    input:
        "qa_data/{lang}/{split}.context.txt"
    output:
        split="qa_data/{lang}/{split}.context.split.txt",
        offsets="qa_data/{lang}/{split}.context.offsets.txt",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from wtpsplit import WtP
        logger = logging.getLogger("split")
        logger.setLevel(logging.INFO)
        logger.info("Loading WtP sentence splitting model.")
        model = WtP("wtp-canine-s-12l")
        model.half().to("cuda")

        lang = wildcards.lang
        if lang == "sw":
            lang = "en"

        logger.info("Model loaded. Splitting sentences.")
        with open(input[0], encoding="utf-8") as f_in, open(output.split, "w", encoding="utf-8") as f_out, \
                open(output.offsets, "w", encoding="utf-8") as f_offsets:
            for line in f_in:
                sentences = model.split(
                    line.strip(), lang_code=lang)
                print("\t".join(sentences), file=f_out)
                offsets = [0]
                for sentence in sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


rule translate_questions:
    input:
        src_text="qa_data/{lang}/{split}.question.txt"
    output:
        tgt_text="qa_data/{lang}/{split}.question.eng"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")
        with open(input.src_text, encoding="utf-8") as f:
            src_text = f.readlines()
        logger.info("Source text loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done. Writing to file.")
        with open(output.tgt_text, "w", encoding="utf-8") as f:
            for line in translation:
                print(line['translation_text'], file=f)
        logger.info("Done.")


rule translate_contexts:
    input:
        src_text="qa_data/{lang}/{split}.context.split.txt"
    output:
        tgt_text="qa_data/{lang}/{split}.context.eng",
        tgt_sentence_offsets="qa_data/{lang}/{split}.context.eng-offsets"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram16G|gpuram24G|gpuram40G|gpuram48G'"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")

        context_offsets = [0]
        src_text = []
        with open(input.src_text, encoding="utf-8") as f:
            for context_line in f:
                context_sentences = context_line.strip().split("\t")
                src_text.extend(context_sentences)
                context_offsets.append(context_offsets[-1] + len(context_sentences))

        logger.info("Source context texts loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done.")

        logger.info("Reorganize back to context and write to file.")
        with open(output.tgt_text, "w", encoding="utf-8") as f_tgt, \
                open(output.tgt_sentence_offsets, "w", encoding="utf-8") as f_offsets:
            for i in range(len(context_offsets) - 1):
                tgt_context_sentences = [
                    line['translation_text'] for line in
                    translation[context_offsets[i]:context_offsets[i+1]]]
                print(" ".join(tgt_context_sentences), file=f_tgt)
                offsets = [0]
                for sentence in tgt_context_sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


def qa_model_dir(wildcards):
    if wildcards.method in ["baseline", "baseline-deberta"]:
        return []
    elif wildcards.method == "finetune":
        return "qa_experiments/finetune/{lang}/model/checkpoints/checkpoint-500"
    else:
        raise ValueError("Unknown method: {}".format(wildcards.method))

rule qa_inference:
    input:
        question="qa_data/{lang}/{split}.question.eng",
        context="qa_data/{lang}/{split}.context.eng",
        context_offsets="qa_data/{lang}/{split}.context.eng-offsets",
        model=qa_model_dir
    output:
        "qa_experiments/{method}/{lang}/{split}.eng-annotated",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from question_answering import question_answering

        if wildcards.method == "baseline":
            model = "deepset/roberta-base-squad2"
        elif wildcards.method == "baseline-deberta":
            model = "deepset/deberta-v3-large-squad2"
        elif wildcards.method == "finetune":
            model = input.model
        else:
            raise ValueError("Unknown method: {}".format(wildcards.method))

        question_answering(
            input.question, input.context, input.context_offsets,
            output[0], model)


rule project_answer_to_eng:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        orig_answer_offsets="qa_data/{lang}/{split}.ref-offsets",
        eng_context_text="qa_data/{lang}/{split}.context.eng",
        eng_context_offsets="qa_data/{lang}/{split}.context.eng-offsets"
    output:
        "qa_data/{lang}/{split}.answers.eng"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram48G'"
    run:

        # This rule takes the original contexts and answer spans, and projects
        # the spans to the English translation of the context.

        # The original context is sentence-split, with tabs as sentence
        # separators. The span are 4 numbers, with start/end sentence/character
        # offsets marking the answer in the context.

        # The main function used here is `project_markup`
        from constrained_translate_with_tags import project_markup

        eng_sentences = []
        orig_sentences = []
        orig_spans = []
        lines = []

        f_orig = open(input.orig_context_text, encoding="utf-8")
        f_eng = open(input.eng_context_text, encoding="utf-8")
        f_offsets = open(input.eng_context_offsets, encoding="utf-8")
        f_answers = open(input.orig_answer_offsets, encoding="utf-8")

        for orig, eng, offsets, answer in zip(f_orig, f_eng, f_offsets, f_answers):
            orig_spl = orig.strip().split("\t")
            offsets_spl = list(map(int, offsets.strip().split(" ")))
            eng_spl = [eng.strip()[start:end] for start, end in zip(offsets_spl[:-1], offsets_spl[1:])]

            sent_beg, ans_beg, sent_end, ans_end = map(int, answer.strip().split("\t"))
            if sent_beg == 0 and sent_end == 0:
                lines.append(False)
                continue

            # from the original context, select just the sentences that contain the answer,
            # keep the answer span
            relevant_sentences = orig_spl[sent_beg:sent_end]
            ans_end_mult = sum(len(sent) for sent in relevant_sentences[:-1]) + ans_end

            orig_sentences.append("".join(relevant_sentences))
            orig_spans.append([("ANSWER", ans_beg, ans_end_mult)])
            eng_sentences.append("".join(eng_spl[sent_beg:sent_end]))
            lines.append(True);

        f_orig.close()
        f_eng.close()
        f_offsets.close()
        f_answers.close()

        projected_spans = project_markup(
            orig_sentences, orig_spans, eng_sentences,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn",
            model="ychenNLP/nllb-200-3.3B-easyproject",
            batch_size=8,
            max_span_len=100,
            is_token_based=False)

        with open(output[0], "w", encoding="utf-8") as f_out:
            pointer = 0
            for has_answer in lines:
                if has_answer:
                    print(projected_spans[pointer])
                    _, answer_start, answer_end = projected_spans[pointer][0]
                    sent = eng_sentences[pointer]
                    print(sent[answer_start:answer_end], file=f_out)
                    pointer += 1
                else:
                    print("No Answer", file=f_out)

rule project_answer_from_eng:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        eng_context_text="qa_data/{lang}/{split}.context.eng",
        eng_context_offsets="qa_data/{lang}/{split}.context.eng-offsets",
        eng_answers="qa_experiments/{method}/{lang}/{split}.eng-annotated"
    output:
        "qa_experiments/{method}/{lang}/{split}.answers"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram48G|gpuram40G|gpuram24G'"
    run:
        from constrained_translate_with_tags import project_markup

        with open(input.orig_context_text, encoding="utf-8") as f_orig:
            orig_context_text = [
                line.strip().split("\t") for line in f_orig.readlines()]
        with open(input.eng_context_text, encoding="utf-8") as f_eng, \
                 open(input.eng_context_offsets, encoding="utf-8") as f_offsets:
            eng_context_text = []
            for text_line, offsets_line in zip(f_eng, f_offsets):
                offsets = list(map(int, offsets_line.strip().split(" ")))
                text_line = text_line.strip()
                eng_context_text.append(
                    [text_line[start:end] for start, end in zip(offsets[:-1], offsets[1:])])

        eng_sentences = []
        tgt_sentences = []
        eng_spans = []
        with open(input.eng_answers, encoding="utf-8") as f_answers:
            for line, orig_context, eng_context in zip(f_answers, orig_context_text, eng_context_text):
                start_sentence_id, answer_start_in_sent, end_sentence_id, answer_end_in_sent = \
                    map(int, line.strip().split("\t"))

                tmp_eng_sentences = eng_context[start_sentence_id:end_sentence_id]
                answer_end = (
                    sum(len(sent) for sent in tmp_eng_sentences[:-1]) # all previous sentences
                    + answer_end_in_sent)

                eng_sentences.append("".join(tmp_eng_sentences))
                eng_spans.append([("ANSWER", answer_start_in_sent, answer_end)])
                tgt_sentences.append("".join(orig_context[start_sentence_id:end_sentence_id]))

        projected_spans = project_markup(
            eng_sentences, eng_spans, tgt_sentences,
            tgt_lang=NLLB_CODES[wildcards.lang],
            model="ychenNLP/nllb-200-3.3B-easyproject",
            batch_size=8,
            max_span_len=100,
            is_token_based=False)

        with open(output[0], "w", encoding="utf-8") as f_out:
            for span, tgt_sent in zip(projected_spans, tgt_sentences):
                _, answer_start, answer_end = span[0]
                print(tgt_sent[answer_start:answer_end], file=f_out)


rule evaluate:
    input:
        correct="qa_data/{lang}/{split}.answers",
        predicted="qa_experiments/{method}/{lang}/{split}.answers"
    output:
        "qa_experiments/{method}/{lang}/{split}.score"
    run:
        import evaluate

        cer_metric = evaluate.load("cer")
        chrf_metric = evaluate.load("chrf")

        with open(input.correct, encoding="utf-8") as f_correct:
            correct = f_correct.read().splitlines()
        with open(input.predicted, encoding="utf-8") as f_predicted:
            predicted = f_predicted.read().splitlines()

        cer_value = cer_metric.compute(predictions=predicted, references=correct)
        chrf_value = chrf_metric.compute(
            predictions=predicted,
            references=[[x] for x in correct])

        with open(output[0], "w", encoding="utf-8") as f_out:
            print(f"{cer_value},{chrf_value['score']}", file=f_out)


rule data_for_qa_finetuning:
    input:
        answers="qa_data/{lang}/{split}.answers.eng",
        questions="qa_data/{lang}/{split}.question.eng",
        contexts="qa_data/{lang}/{split}.context.eng",
    output:
        "qa_data/{lang}/{split}.qa_eng_finetune.tsv"
    shell:
        "./prepare_qa_datasets.sh {input.questions} {input.contexts} {input.answers} {output}"

rule qa_finetuning:
    input:
        train="qa_data/{lang}/train.qa_eng_finetune.tsv",
        valid="qa_data/{lang}/validation.qa_eng_finetune.tsv"
    output:
        directory("qa_experiments/{lang}/finetuning/checkpoints"),
        touch("qa_experiments/{lang}/finetuning/success")
    params:
        batch_size=16,
        max_epochs=10,
        learning_rate=2e-5,
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram24G|gpuram40G|gpuram48G'"
    run:
        from finetune_qa import finetune_qa
        finetune_qa(input.train,
                    input.valid,
                    output[0],
                    model_type="deepset/roberta-large-squad2",
                    batch_size=params.batch_size,
                    epochs=params.max_epochs,
                    learning_rate=params.learning_rate)



rule reference_answer_offsets:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        orig_answers="qa_data/{lang}/{split}.answers"
    output:
        "qa_data/{lang}/{split}.ref-offsets"
    run:
        from get_answer_offsets import get_answer_offsets

        with open(input.orig_context_text, encoding="utf-8") as f_orig, \
                open(input.orig_answers, encoding="utf-8") as f_answers, \
                open(output[0], "w", encoding="utf-8") as f_out:
            for orig_context, orig_answers in zip(f_orig, f_answers):
                offsets = get_answer_offsets(orig_context.strip(), orig_answers.strip())
                print("\t".join(map(str, offsets)), file=f_out)


rule reconstruct_answer_from_offsets:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        ref_offsets="qa_data/{lang}/{split}.ref-offsets"
    output:
        "qa_data/{lang}/{split}.reconstructed"
    run:
        from get_answer_offsets import reconstruct_answer

        with open(input.orig_context_text, encoding="utf-8") as f_orig, \
                open(input.ref_offsets, encoding="utf-8") as f_offsets, \
                open(output[0], "w", encoding="utf-8") as f_out:
            for orig_context, offsets in zip(f_orig, f_offsets):
                offsets = list(map(int, offsets.strip().split("\t")))
                print(reconstruct_answer(orig_context.strip(), offsets), file=f_out)
