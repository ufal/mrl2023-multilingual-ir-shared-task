import logging

QA_LANGUAGES = [
    "ar", "bn", "en", "fi", "id", 'ko', 'ru', 'sw', 'te'
]

QA_VAL_LANGUAGES = {
    "al": "ALS",
    "az": "AZ",
    "id": "ID",
    "tr": "TR",
    "uz": "UZ",
    "yo": "YO"
}

NLLB_CODES = {
    "ar": "arb_Arab",
    "bn": "ben_Beng",
    "en": "eng_Latn",
    "fi": "fin_Latn",
    "id": "ind_Latn",
    'ko': "kor_Hang",
    'ru': "rus_Cyrl",
    'sw': "swh_Latn",
    'te': "tel_Telu",

    "al": "deu_Latn",
    "az": "azj_Latn",
    "bn": "ben_Beng",
    "id": "ind_Latn",
    "tr": "tur_Latn",
    "uz": "uzn_Latn",
    "yo": "yor_Latn"
}


localrules: data_for_qa_finetuning

QA_TEMPLATE_GRID_SEARCH_NAME = (
    "qa_experiments/finetune/exp"
    "-gradsteps{gradient_accumulation_steps}"
    "-lr{learning_rate}"
    "-wd{weight_decay}"
    "-gradnorm{max_grad_norm}"
    "-warmup{warmup_ratio}/checkpoints")

def generate_configurations_for_grid_search():
    for gradient_accumulation_steps in [1]:
        for learning_rate in [5e-6, 1e-6, 1e-7]:
            for weight_decay in [0.1]:
                for max_grad_norm in [1]:
                    for warmup_ratio in [0.0, 0.5]:
                        yield dict(gradient_accumulation_steps=gradient_accumulation_steps,
                                   learning_rate=learning_rate,
                                   weight_decay=weight_decay,
                                   max_grad_norm=max_grad_norm,
                                   warmup_ratio=warmup_ratio)

def grid_search(wildcards):
    return [QA_TEMPLATE_GRID_SEARCH_NAME.format(**cfg) for cfg in generate_configurations_for_grid_search()]

rule all:
    input:
        #expand("qa_experiments/baseline/{lang}/{split}.score",
        #       lang=QA_LANGUAGES, split=["validation", "test"]),
        #expand("qa_data/{lang}/train.{type}.eng",
        #       lang=QA_LANGUAGES, type=["question", "context"]),
        #"qa_experiments/baseline/en/tiny.answers",
        #expand("qa_data/{lang}/{split}.answers.eng",
        #       lang=QA_LANGUAGES, split=["train", "validation", "test"])
        #"qa_experiments/en/finetune/success"
        expand("qa_experiments/finetune/{lang}/{split}.answer-candidates", lang=QA_LANGUAGES, split=["validation", "test"])
        #grid_search



rule download_xtreme_up:
    output:
        expand("xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl",
               split=['train', 'dev', 'test'],
               lang=QA_LANGUAGES)
    shell:
        """
        wget https://storage.googleapis.com/xtreme-up/xtreme-up-v1.1.jsonl.tgz
        tar -xzf xtreme-up-v1.1.jsonl.tgz
        rm xtreme-up-v1.1.jsonl.tgz
        """

rule format_xtreme_up_qa:
    input:
        "xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl"
    output:
        question="qa_data/{lang}/{split}.question.txt",
        context="qa_data/{lang}/{split}.context.txt",
        anwser="qa_data/{lang}/{split}.answers",
    run:
        import json
        with open(input[0], encoding="utf-8") as f:
            data = [json.loads(line) for line in f]
        with open(output.question, "w", encoding="utf-8") as f_question, \
             open(output.context, "w", encoding="utf-8") as f_context, \
             open(output.anwser, "w", encoding="utf-8") as f_answer:
            for item in data:
                print(item['question'], file=f_question)
                print(f"{item['title']}. {item['context']}", file=f_context)
                print(item['target'], file=f_answer)

rule download_validation:
    output:
        expand("task_data/MRL_ST_2023/Val/QA_{lang}_Val.csv", lang=QA_VAL_LANGUAGES.keys())
    shell:
        """
        mkdir -p task_data/
        cd task_data
        gdown --folder https://drive.google.com/drive/folders/15exVuU06dfbUWQtvSkRpA4JG4sxzgVli?usp=share_link
        """

rule format_validation:
    input:
        lambda wildcards: "task_data/MRL_ST_2023/Val/QA_{}_Val.csv".format(QA_VAL_LANGUAGES[wildcards.lang])
    output:
        context="qa_data/{lang}/task_val.context.txt",
        question="qa_data/{lang}/task_val.question.txt",
        anwser="qa_data/{lang}/task_val.answers"
    run:
        import csv

        f_input = open(input[0], encoding="utf-8", newline="")
        f_context = open(output.context, "w", encoding="utf-8")
        f_question = open(output.question, "w", encoding="utf-8")
        f_answer = open(output.anwser, "w", encoding="utf-8")

        for row in csv.reader(f_input):
            print(row[1], file=f_context)
            print(row[2], file=f_question)
            print(row[3], file=f_answer)

        f_input.close()
        f_context.close()
        f_question.close()
        f_answer.close()


rule split_contexts_to_sentences:
    input:
        "qa_data/{lang}/{split}.context.txt"
    output:
        split="qa_data/{lang}/{split}.context.split.txt",
        offsets="qa_data/{lang}/{split}.context.offsets.txt",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from wtpsplit import WtP
        logger = logging.getLogger("split")
        logger.setLevel(logging.INFO)
        logger.info("Loading WtP sentence splitting model.")
        model = WtP("wtp-canine-s-12l")
        model.half().to("cuda")

        lang = wildcards.lang
        if lang == "sw":
            lang = "en"

        logger.info("Model loaded. Splitting sentences.")
        with open(input[0], encoding="utf-8") as f_in, open(output.split, "w", encoding="utf-8") as f_out, \
                open(output.offsets, "w", encoding="utf-8") as f_offsets:
            for line in f_in:
                sentences = model.split(
                    line.strip(), lang_code=lang)
                print("\t".join(sentences), file=f_out)
                offsets = [0]
                for sentence in sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


rule translate_questions:
    input:
        src_text="qa_data/{lang}/{split}.question.txt"
    output:
        tgt_text=protected("qa_data/{lang}/{split}.question.eng")
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")
        with open(input.src_text, encoding="utf-8") as f:
            src_text = f.readlines()
        logger.info("Source text loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done. Writing to file.")
        with open(output.tgt_text, "w", encoding="utf-8") as f:
            for line in translation:
                print(line['translation_text'], file=f)
        logger.info("Done.")


rule translate_contexts:
    input:
        src_text="qa_data/{lang}/{split}.context.split.txt"
    output:
        tgt_text=protected("qa_data/{lang}/{split}.context.eng"),
        tgt_sentence_offsets=protected("qa_data/{lang}/{split}.context.eng-offsets")
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        #flags="--gres=gpu:1 --constraint='gpuram16G|gpuram24G|gpuram40G|gpuram48G'"
        flags="--gres=gpu:1 --constraint='gpuram24G|gpuram40G|gpuram48G'"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")

        context_offsets = [0]
        src_text = []
        with open(input.src_text, encoding="utf-8") as f:
            for context_line in f:
                context_sentences = context_line.strip().split("\t")
                src_text.extend(context_sentences)
                context_offsets.append(context_offsets[-1] + len(context_sentences))

        logger.info("Source context texts loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang[:2]],
            tgt_lang="eng_Latn")
        logger.info("Translation done.")

        logger.info("Reorganize back to context and write to file.")
        with open(output.tgt_text, "w", encoding="utf-8") as f_tgt, \
                open(output.tgt_sentence_offsets, "w", encoding="utf-8") as f_offsets:
            for i in range(len(context_offsets) - 1):
                tgt_context_sentences = [
                    line['translation_text'] for line in
                    translation[context_offsets[i]:context_offsets[i+1]]]
                print(" ".join(tgt_context_sentences), file=f_tgt)
                offsets = [0]
                for sentence in tgt_context_sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


def qa_model_dir(wildcards):
    if wildcards.method in ["baseline", "baseline-deberta"]:
        return []
    elif wildcards.method == "finetune":
        return "qa_experiments/finetune/best"
    else:
        raise ValueError("Unknown method: {}".format(wildcards.method))

rule qa_inference:
    input:
        question="qa_data/{lang}/{split}.question.eng",
        context="qa_data/{lang}/{split}.context.eng",
        context_offsets="qa_data/{lang}/{split}.context.eng-offsets",
        model=qa_model_dir
    output:
        "qa_experiments/{method}/{lang}/{split}.eng-annotated",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from question_answering import question_answering

        if wildcards.method == "baseline":
            model = "deepset/roberta-large-squad2"
        elif wildcards.method == "baseline-deberta":
            model = "deepset/deberta-v3-large-squad2"
        elif wildcards.method == "finetune":
            model = input.model
        else:
            raise ValueError("Unknown method: {}".format(wildcards.method))

        question_answering(
            input.question, input.context, input.context_offsets,
            output[0], model)


rule project_answer_to_eng:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        orig_answer_offsets="qa_data/{lang}/{split}.ref-offsets",
        eng_context_text="qa_data/{lang}/{split}.context.eng",
        eng_context_offsets="qa_data/{lang}/{split}.context.eng-offsets"
    output:
        protected("qa_data/{lang}/{split}.answers.eng")
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram48G'"
    run:

        # This rule takes the original contexts and answer spans, and projects
        # the spans to the English translation of the context.

        # The original context is sentence-split, with tabs as sentence
        # separators. The span are 4 numbers, with start/end sentence/character
        # offsets marking the answer in the context.

        # The main function used here is `project_markup`
        from constrained_translate_with_tags import project_markup

        eng_sentences = []
        orig_sentences = []
        orig_spans = []
        lines = []

        f_orig = open(input.orig_context_text, encoding="utf-8")
        f_eng = open(input.eng_context_text, encoding="utf-8")
        f_offsets = open(input.eng_context_offsets, encoding="utf-8")
        f_answers = open(input.orig_answer_offsets, encoding="utf-8")

        for orig, eng, offsets, answer in zip(f_orig, f_eng, f_offsets, f_answers):
            orig_spl = orig.strip().split("\t")
            offsets_spl = list(map(int, offsets.strip().split(" ")))
            eng_spl = [eng.strip()[start:end] for start, end in zip(offsets_spl[:-1], offsets_spl[1:])]

            sent_beg, ans_beg, sent_end, ans_end = map(int, answer.strip().split("\t"))
            if sent_beg == 0 and sent_end == 0:
                lines.append(False)
                continue

            # from the original context, select just the sentences that contain the answer,
            # keep the answer span
            relevant_sentences = orig_spl[sent_beg:sent_end]
            ans_end_mult = sum(len(sent) for sent in relevant_sentences[:-1]) + ans_end

            orig_sentences.append("".join(relevant_sentences))
            orig_spans.append([("ANSWER", ans_beg, ans_end_mult)])
            eng_sentences.append("".join(eng_spl[sent_beg:sent_end]))
            lines.append(True);

        f_orig.close()
        f_eng.close()
        f_offsets.close()
        f_answers.close()

        projected_spans = project_markup(
            orig_sentences, orig_spans, eng_sentences,
            src_lang=NLLB_CODES[wildcards.lang[:2]],
            tgt_lang="eng_Latn",
            model="ychenNLP/nllb-200-3.3B-easyproject",
            batch_size=8,
            max_span_len=100,
            is_token_based=False)

        with open(output[0], "w", encoding="utf-8") as f_out:
            pointer = 0
            for has_answer in lines:
                if has_answer:
                    print(projected_spans[pointer])
                    _, answer_start, answer_end = projected_spans[pointer][0]
                    sent = eng_sentences[pointer]
                    print(sent[answer_start:answer_end], file=f_out)
                    pointer += 1
                else:
                    print("No Answer", file=f_out)

rule project_answer_from_eng:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        eng_context_text="qa_data/{lang}/{split}.context.eng",
        eng_context_offsets="qa_data/{lang}/{split}.context.eng-offsets",
        eng_answers="qa_experiments/{method}/{lang}/{split}.eng-annotated"
    output:
        protected("qa_experiments/{method}/{lang}/{split}.answer-candidates")
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram48G'"
    run:
        from constrained_translate_with_tags import project_markup

        with open(input.orig_context_text, encoding="utf-8") as f_orig:
            orig_context_text = [
                line.strip().split("\t") for line in f_orig.readlines()]
        with open(input.eng_context_text, encoding="utf-8") as f_eng, \
                 open(input.eng_context_offsets, encoding="utf-8") as f_offsets:
            eng_context_text = []
            for text_line, offsets_line in zip(f_eng, f_offsets):
                offsets = list(map(int, offsets_line.strip().split(" ")))
                text_line = text_line.strip()
                eng_context_text.append(
                    [text_line[start:end] for start, end in zip(offsets[:-1], offsets[1:])])

        eng_sentences = []
        tgt_sentences = []
        eng_spans = []
        with open(input.eng_answers, encoding="utf-8") as f_answers:
            for line, orig_context, eng_context in zip(f_answers, orig_context_text, eng_context_text):
                start_sentence_id, answer_start_in_sent, end_sentence_id, answer_end_in_sent = \
                    map(int, line.strip().split("\t"))

                tmp_eng_sentences = eng_context[start_sentence_id:end_sentence_id]
                answer_end = (
                    sum(len(sent) for sent in tmp_eng_sentences[:-1]) # all previous sentences
                    + answer_end_in_sent)

                eng_sentences.append("".join(tmp_eng_sentences))
                eng_spans.append([("ANSWER", answer_start_in_sent, answer_end)])
                tgt_sentences.append("".join(orig_context[start_sentence_id:end_sentence_id]))

        projected_spans = project_markup(
            eng_sentences, eng_spans, tgt_sentences,
            tgt_lang=NLLB_CODES[wildcards.lang],
            model="ychenNLP/nllb-200-3.3B-easyproject",
            batch_size=8,
            max_span_len=100,
            is_token_based=False)

        with open(output[0], "w", encoding="utf-8") as f_out:
            for span, tgt_sent in zip(projected_spans, tgt_sentences):
                _, answer_start, answer_end = span[0]
                print(tgt_sent[answer_start:answer_end], file=f_out)


rule evaluate:
    input:
        correct="qa_data/{lang}/{split}.answers",
        predicted="qa_experiments/{method}/{lang}/{split}.answers"
    output:
        "qa_experiments/{method}/{lang}/{split}.score"
    run:
        import evaluate

        cer_metric = evaluate.load("cer")
        chrf_metric = evaluate.load("chrf")

        with open(input.correct, encoding="utf-8") as f_correct:
            correct = f_correct.read().splitlines()
        with open(input.predicted, encoding="utf-8") as f_predicted:
            predicted = f_predicted.read().splitlines()

        cer_value = cer_metric.compute(predictions=predicted, references=correct)
        chrf_value = chrf_metric.compute(
            predictions=predicted,
            references=[[x] for x in correct])

        with open(output[0], "w", encoding="utf-8") as f_out:
            print(f"{cer_value},{chrf_value['score']}", file=f_out)


rule data_for_qa_finetuning:
    input:
        answers=expand("qa_data/{lang}/{{split}}.answers.eng", lang=QA_LANGUAGES),
        questions=expand("qa_data/{lang}/{{split}}.question.eng", lang=QA_LANGUAGES),
        contexts=expand("qa_data/{lang}/{{split}}.context.eng", lang=QA_LANGUAGES)
    output:
        "qa_data/{split}.qa_eng_finetune.tsv"
    shell:
        """
        echo -e "question\tcontext\tanswer" > {output}
        paste <(cat {input.questions}) <(cat {input.contexts}) <(cat {input.answers}) \
          | grep -v 'No Answer$' \
          | grep -Pv '\t$' \
          | grep -Pv '\t\t' \
          | grep -Pv '^\t' \
            >> {output}
        """

rule qa_finetuning:
    input:
        train="qa_data/train.qa_eng_finetune.tsv",
        valid="qa_data/validation.qa_eng_finetune.tsv"
    output:
        protected(directory(QA_TEMPLATE_GRID_SEARCH_NAME))
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram24G|gpuram40G|gpuram48G'"
    params:
        epochs=5
    run:
        from finetune_qa import finetune_qa
        finetune_qa(input.train,
                    input.valid,
                    output[0],
                    model_type="deepset/roberta-large-squad2",
                    gradient_accumulation_steps=int(wildcards.gradient_accumulation_steps),
                    epochs=params.epochs,
                    learning_rate=float(wildcards.learning_rate),
                    weight_decay=float(wildcards.weight_decay),
                    max_grad_norm=float(wildcards.max_grad_norm),
                    warmup_ratio=float(wildcards.warmup_ratio))


# rule qa_best_model:
#     input:
#         grid_search
#     output:
#         "qa_experiments/finetune/best"
#     shell:
#         """
#         echo "TODO: implement"
#         """


rule reference_answer_offsets:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        orig_answers="qa_data/{lang}/{split}.answers"
    output:
        "qa_data/{lang}/{split}.ref-offsets"
    run:
        from get_answer_offsets import get_answer_offsets

        with open(input.orig_context_text, encoding="utf-8") as f_orig, \
                open(input.orig_answers, encoding="utf-8") as f_answers, \
                open(output[0], "w", encoding="utf-8") as f_out:
            for orig_context, orig_answers in zip(f_orig, f_answers):
                offsets = get_answer_offsets(orig_context.strip(), orig_answers.strip())
                print("\t".join(map(str, offsets)), file=f_out)


rule reconstruct_answer_from_offsets:
    input:
        orig_context_text="qa_data/{lang}/{split}.context.split.txt",
        ref_offsets="qa_data/{lang}/{split}.ref-offsets"
    output:
        "qa_data/{lang}/{split}.reconstructed"
    run:
        from get_answer_offsets import reconstruct_answer

        with open(input.orig_context_text, encoding="utf-8") as f_orig, \
                open(input.ref_offsets, encoding="utf-8") as f_offsets, \
                open(output[0], "w", encoding="utf-8") as f_out:
            for orig_context, offsets in zip(f_orig, f_offsets):
                offsets = list(map(int, offsets.strip().split("\t")))
                print(reconstruct_answer(orig_context.strip(), offsets), file=f_out)

rule prepare_data_noanswer_classifier:
    input:
        contexts=expand("qa_data/{lang}/{{split}}.context.eng", lang=QA_LANGUAGES),
        questions=expand("qa_data/{lang}/{{split}}.question.eng", lang=QA_LANGUAGES),
        answers=expand("qa_data/{lang}/{{split}}.answers", lang=QA_LANGUAGES)
    output:
        "qa_data/{split}-all.eng.tsv"
    shell:
        """
        echo -e "text\tlabel" > {output}

        paste <(paste -d" " <(cat {input.questions}) <(cat {input.contexts})) \
              <(cat {input.answers} | \
                awk '{{if ($0 == "No Answer") {{print "0"}} else {{print "1"}} }}') \
              >> {output}
        """

rule sample_noanswer_validation:
    input:
        "qa_data/validation-all.eng.tsv"
    output:
        "qa_data/validation-sample.eng.tsv"
    params:
        size=1000
    shell:
        """
        head -n1 {input} > {output}
        tail -n+2 {input} | shuf -n {params.size} >> {output}
        """

rule train_noanswer_classifier:
    input:
        train="qa_data/train-all.eng.tsv",
        valid="qa_data/validation-sample.eng.tsv"
    output:
        directory("qa_experiments/noanswer_model"),
        "qa_experiments/noanswer_model/best_checkpoint"
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    params:
        model="deepset/roberta-large-squad2",
        epochs=5,
        learning_rate=1e-5
    run:
        from noanswer_train import train_noanswer_classification
        train_noanswer_classification(train_tsv=input.train,
                                      valid_tsv=input.valid,
                                      output_dir=output[0],
                                      model_type=params.model,
                                      epochs=params.epochs,
                                      learning_rate=params.learning_rate)

rule classify_noanswers:
    input:
        model="qa_experiments/noanswer_model",
        contexts="qa_data/{lang}/{split}.context.eng",
        questions="qa_data/{lang}/{split}.question.eng"
    output:
        "qa_experiments/noanswer_model/{lang}/{split}.noanswers"
    params:
        batch_size=16
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from noanswer_classify import noanswer_classify
        noanswer_classify(model_path=input.model,
                          question_file=input.questions,
                          context_file=input.contexts,
                          output_path=output[0],
                          batch_size=params.batch_size)

rule merge_answers_and_noanswers:
    input:
        candidates="qa_experiments/{method}/{lang}/{split}.answer-candidates",
        noanswers="qa_experiments/noanswer_model/{lang}/{split}.noanswers"
    output:
        "qa_experiments/{method}/{lang}/{split}.answers"
    run:
        cand_f = open(input.candidates, encoding="utf-8")
        classes_f = open(input.noanswers, encoding="utf-8")
        out_f = open(output[0], "w", encoding="utf-8")

        for cand, cls in zip(cand_f, classes_f):
            if cls.strip() == "LABEL_0":
                print("No Answer", file=out_f)
            else:
                print(cand.strip(), file=out_f)
