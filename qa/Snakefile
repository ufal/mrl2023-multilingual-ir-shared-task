
NLLB_CODES = {
    "ar": "ara_Arab",
    "bn": "ben_Beng",
    "en": "eng_Latn",
    "fi": "fin_Latn",
    "id": "ind_Latn",
    'ko': "kor_Hang",
    'ru': "rus_Cyrl",
    'sw': "swh_Latn",
    'te': "tel_Telu",
}


rule all:
    input:
        expand("qa_data/{lang}/{type}.{file}.eng",
               lang=QA_LANGUAGES,
               type=["train", "validation", "test"],
               file=["context", "question"]),


rule download_xtreme_up:
    output:
        expand("xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl",
               split=['train', 'dev', 'test'],
               lang=QA_LANGUAGES)
    shell:
        """
        wget https://storage.googleapis.com/xtreme-up/xtreme-up-v1.1.jsonl.tgz
        tar -xzf xtreme-up-v1.1.jsonl.tgz
        """


rule format_xtreme_up_qa:
    input:
        "xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl"
    output:
        question="qa_data/{lang}/{split}.question.txt",
        context="qa_data/{lang}/{split}.context.txt",
        anwser="qa_data/{lang}/{split}.answers",
    run:
        import json
        with open(input[0]) as f:
            data = [json.loads(line) for line in f]
        with open(output.question, "w") as f_question, \
             open(output.context, "w") as f_context, \
             open(output.anwser, "w") as f_answer:
            for item in data:
                print(item['question'], file=f_question)
                print(f"{item['title']}. {item['context']}", file=f_context)
                print(item['target'], file=f_answer)


rule sentence_split_context:
    input:
        "qa_data/{lang}/{split}.context.txt"
    output:
        split="qa_data/{lang}/{split}.context.split.txt",
        offsets="qa_data/{lang}/{split}.context.offsets.txt",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from wtpsplit import WtP
        logger = logging.getLogger("split")
        logger.setLevel(logging.INFO)
        logger.info("Loading WtP sentence splitting model.")
        model = WtP("wtp-canine-s-12l")
        model.half().to("cuda")

        lang = wildcards.lang
        if lang == "sw":
            lang = "en"

        logger.info("Model loaded. Splitting sentences.")
        with open(input[0]) as f_in, open(output.split, "w") as f_out, \
                open(output.offsets, "w") as f_offsets:
            for line in f_in:
                sentences = model.split(
                    line.strip(), lang_code=lang)
                print("\t".join(sentences), file=f_out)
                offsets = [0]
                for sentence in sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


rule translate_input_text:
    input:
        question="qa_data/{lang}/{split}.question.eng",
        context="qa_data/{lang}/{split}.context.eng",
    output:
        "qa_experiments/baseline/{lang}/{split}.eng_annotated",
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        import question_answering

        results = question_answering.predict(
            model="deepset/deberta-v3-base-squad2",
            questions=open(input.question).read().splitlines(),
            contexts=open(input.context).read().splitlines())
