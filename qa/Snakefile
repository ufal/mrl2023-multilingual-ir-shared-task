import logging

QA_LANGUAGES = [
    "ar", "bn", "en", "fi", "id", 'ko', 'ru', 'sw', 'te'
]

NLLB_CODES = {
    "ar": "arb_Arab",
    "bn": "ben_Beng",
    "en": "eng_Latn",
    "fi": "fin_Latn",
    "id": "ind_Latn",
    'ko': "kor_Hang",
    'ru': "rus_Cyrl",
    'sw': "swh_Latn",
    'te': "tel_Telu",
}


rule all:
    input:
        expand("qa_data/{lang}/{type}.{file}.eng",
               lang=QA_LANGUAGES,
               type=["train", "validation", "test"],
               file=["context", "question"]),


rule download_xtreme_up:
    output:
        expand("xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl",
               split=['train', 'dev', 'test'],
               lang=QA_LANGUAGES)
    shell:
        """
        wget https://storage.googleapis.com/xtreme-up/xtreme-up-v1.1.jsonl.tgz
        tar -xzf xtreme-up-v1.1.jsonl.tgz
        """


rule format_xtreme_up_qa:
    input:
        "xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl"
    output:
        question="qa_data/{lang}/{split}.question.txt",
        context="qa_data/{lang}/{split}.context.txt",
        anwser="qa_data/{lang}/{split}.answers",
    run:
        import json
        with open(input[0]) as f:
            data = [json.loads(line) for line in f]
        with open(output.question, "w") as f_question, \
             open(output.context, "w") as f_context, \
             open(output.anwser, "w") as f_answer:
            for item in data:
                print(item['question'], file=f_question)
                print(f"{item['title']}. {item['context']}", file=f_context)
                print(item['target'], file=f_answer)


rule sentence_split_context:
    input:
        "qa_data/{lang}/{split}.context.txt"
    output:
        split="qa_data/{lang}/{split}.context.split.txt",
        offsets="qa_data/{lang}/{split}.context.offsets.txt",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from wtpsplit import WtP
        logger = logging.getLogger("split")
        logger.setLevel(logging.INFO)
        logger.info("Loading WtP sentence splitting model.")
        model = WtP("wtp-canine-s-12l")
        model.half().to("cuda")

        lang = wildcards.lang
        if lang == "sw":
            lang = "en"

        logger.info("Model loaded. Splitting sentences.")
        with open(input[0]) as f_in, open(output.split, "w") as f_out, \
                open(output.offsets, "w") as f_offsets:
            for line in f_in:
                sentences = model.split(
                    line.strip(), lang_code=lang)
                print("\t".join(sentences), file=f_out)
                offsets = [0]
                for sentence in sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


rule translate_input_text:
    input:
        src_text="{dataset}_data/{lang}/{split}.question.txt"
    output:
        tgt_text="{dataset}_data/{lang}/{split}.question.eng"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")
        with open(input.src_text) as f:
            src_text = f.readlines()
        logger.info("Source text loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done. Writing to file.")
        with open(output.tgt_text, "w") as f:
            for line in translation:
                print(line['translation_text'], file=f)
        logger.info("Done.")


rule translate_context_text:
    input:
        src_text="{dataset}_data/{lang}/{split}.context.split.txt"
    output:
        tgt_text="{dataset}_data/{lang}/{split}.context.eng",
        tgt_sentence_offsets="{dataset}_data/{lang}/{split}.context.eng-offsets"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")

        context_offsets = [0]
        src_text = []
        with open(input.src_text) as f:
            for context_line in f:
                context_sentences = context_line.strip().split("\t")
                src_text.extend(context_sentences)
                context_offsets.append(context_offsets[-1] + len(context_sentences))

        logger.info("Source context texts loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done.")

        logger.info("Reorganize back to context and write to file.")
        with open(output.tgt_text, "w") as f_tgt, \
                open(output.tgt_sentence_offsets, "w") as f_offsets:
            for i in range(len(context_offsets) - 1):
                tgt_context_sentences = [
                    line['translation_text'] for line in
                    translation[context_offsets[i]:context_offsets[i+1]]]
                print(" ".join(tgt_context_sentences), file=f_tgt)
                offsets = [0]
                for sentence in tgt_context_sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")
