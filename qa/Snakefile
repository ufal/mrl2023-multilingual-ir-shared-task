import logging

QA_LANGUAGES = [
    "ar", "bn", "en", "fi", "id", 'ko', 'ru', 'sw', 'te'
]

NLLB_CODES = {
    "ar": "arb_Arab",
    "bn": "ben_Beng",
    "en": "eng_Latn",
    "fi": "fin_Latn",
    "id": "ind_Latn",
    'ko': "kor_Hang",
    'ru': "rus_Cyrl",
    'sw': "swh_Latn",
    'te': "tel_Telu",
}


rule all:
    input:
        expand("qa_experiments/baseline-deberta/{lang}/{split}.score",
               lang=QA_LANGUAGES, split=["validation", "test"]),


rule download_xtreme_up:
    output:
        expand("xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl",
               split=['train', 'dev', 'test'],
               lang=QA_LANGUAGES)
    shell:
        """
        wget https://storage.googleapis.com/xtreme-up/xtreme-up-v1.1.jsonl.tgz
        tar -xzf xtreme-up-v1.1.jsonl.tgz
        rm xtreme-up-v1.1.jsonl.tgz
        """


rule format_xtreme_up_qa:
    input:
        "xtreme_up_v1.1/qa_in_lang/{split}/{lang}.jsonl"
    output:
        question="qa_data/{lang}/{split}.question.txt",
        context="qa_data/{lang}/{split}.context.txt",
        anwser="qa_data/{lang}/{split}.answers",
    run:
        import json
        with open(input[0]) as f:
            data = [json.loads(line) for line in f]
        with open(output.question, "w") as f_question, \
             open(output.context, "w") as f_context, \
             open(output.anwser, "w") as f_answer:
            for item in data:
                print(item['question'], file=f_question)
                print(f"{item['title']}. {item['context']}", file=f_context)
                print(item['target'], file=f_answer)


rule sentence_split_context:
    input:
        "qa_data/{lang}/{split}.context.txt"
    output:
        split="qa_data/{lang}/{split}.context.split.txt",
        offsets="qa_data/{lang}/{split}.context.offsets.txt",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from wtpsplit import WtP
        logger = logging.getLogger("split")
        logger.setLevel(logging.INFO)
        logger.info("Loading WtP sentence splitting model.")
        model = WtP("wtp-canine-s-12l")
        model.half().to("cuda")

        lang = wildcards.lang
        if lang == "sw":
            lang = "en"

        logger.info("Model loaded. Splitting sentences.")
        with open(input[0]) as f_in, open(output.split, "w") as f_out, \
                open(output.offsets, "w") as f_offsets:
            for line in f_in:
                sentences = model.split(
                    line.strip(), lang_code=lang)
                print("\t".join(sentences), file=f_out)
                offsets = [0]
                for sentence in sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")


rule translate_input_text:
    input:
        src_text="{dataset}_data/{lang}/{split}.question.txt"
    output:
        tgt_text="{dataset}_data/{lang}/{split}.question.eng"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")
        with open(input.src_text) as f:
            src_text = f.readlines()
        logger.info("Source text loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done. Writing to file.")
        with open(output.tgt_text, "w") as f:
            for line in translation:
                print(line['translation_text'], file=f)
        logger.info("Done.")


rule translate_context_text:
    input:
        src_text="{dataset}_data/{lang}/{split}.context.split.txt"
    output:
        tgt_text="{dataset}_data/{lang}/{split}.context.eng",
        tgt_sentence_offsets="{dataset}_data/{lang}/{split}.context.eng-offsets"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from transformers import pipeline
        import torch

        logger = logging.getLogger("translate")
        logger.setLevel(logging.INFO)

        logger.info("Loading model.")
        pipe = pipeline(
            "translation", model="facebook/nllb-200-3.3B", device=0)
        logger.info("Model loaded. Loading source text.")

        context_offsets = [0]
        src_text = []
        with open(input.src_text) as f:
            for context_line in f:
                context_sentences = context_line.strip().split("\t")
                src_text.extend(context_sentences)
                context_offsets.append(context_offsets[-1] + len(context_sentences))

        logger.info("Source context texts loaded. Translating.")
        translation = pipe(
            src_text,
            src_lang=NLLB_CODES[wildcards.lang],
            tgt_lang="eng_Latn")
        logger.info("Translation done.")

        logger.info("Reorganize back to context and write to file.")
        with open(output.tgt_text, "w") as f_tgt, \
                open(output.tgt_sentence_offsets, "w") as f_offsets:
            for i in range(len(context_offsets) - 1):
                tgt_context_sentences = [
                    line['translation_text'] for line in
                    translation[context_offsets[i]:context_offsets[i+1]]]
                print(" ".join(tgt_context_sentences), file=f_tgt)
                offsets = [0]
                for sentence in tgt_context_sentences:
                    offsets.append(offsets[-1] + len(sentence) + 1)
                print(" ".join(map(str, offsets)), file=f_offsets)
        logger.info("Done.")



rule baseline_qa:
    input:
        question="{dataset}_data/{lang}/{split}.question.eng",
        context="{dataset}_data/{lang}/{split}.context.eng",
        context_offsets="{dataset}_data/{lang}/{split}.context.eng-offsets"
    output:
        "{dataset}_experiments/baseline/{lang}/{split}.eng-annotated",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from question_answering import question_answering

        question_answering(
            input.question, input.context, input.context_offsets,
            output[0], "deepset/roberta-large-squad2")


rule baseline_deberta:
    input:
        question="{dataset}_data/{lang}/{split}.question.eng",
        context="{dataset}_data/{lang}/{split}.context.eng",
        context_offsets="{dataset}_data/{lang}/{split}.context.eng-offsets"
    output:
        "{dataset}_experiments/baseline-deberta/{lang}/{split}.eng-annotated",
    resources:
        mem="10G",
        cpus_per_task=2,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1"
    run:
        from question_answering import question_answering

        question_answering(
            input.question, input.context, input.context_offsets,
            output[0], "deepset/deberta-v3-large-squad2")


rule project_answer_from_eng:
    input:
        orig_context_text="{dataset}_data/{lang}/{split}.context.split.txt",
        eng_answers="{dataset}_experiments/{method}/{lang}/{split}.eng-annotated"
    output:
        "{dataset}_experiments/{method}/{lang}/{split}.answers"
    resources:
        mem="40G",
        cpus_per_task=8,
        partition="gpu-troja,gpu-ms",
        flags="--gres=gpu:1 --constraint='gpuram48G'"
    run:
        from constrained_translate_with_tags import project_markup

        with open(input.orig_context_text) as f_orig:
            orig_context_text = [
                line.strip().split("\t") for line in f_orig.readlines()]

        eng_sentences = []
        tgt_sentences = []
        eng_spans = []
        with open(input.eng_answers) as f_answers:
            for line, orig_context in zip(f_answers, orig_context_text):
                sentence, sentence_id, start, end = line.strip().split("\t")
                eng_sentences.append(sentence)
                eng_spans.append([("ANSWER", int(start), int(end))])
                tgt_sentences.append(orig_context[int(sentence_id)])

        projected_spans = project_markup(
            eng_sentences, eng_spans, tgt_sentences,
            tgt_lang=NLLB_CODES[wildcards.lang],
            model="ychenNLP/nllb-200-3.3B-easyproject",
            batch_size=8,
            max_span_len=100,
            is_token_based=False)

        with open(output[0], "w") as f_out:
            for span, tgt_sent in zip(projected_spans, tgt_sentences):
                _, answer_start, answer_end = span[0]
                print(tgt_sent[answer_start:answer_end], file=f_out)


rule evaluate:
    input:
        correct="{dataset}_data/{lang}/{split}.answers",
        predicted="{dataset}_experiments/{method}/{lang}/{split}.answers"
    output:
        "{dataset}_experiments/{method}/{lang}/{split}.score"
    run:
        import evaluate

        cer_metric = evaluate.load("cer")
        chrf_metric = evaluate.load("chrf")

        with open(input.correct) as f_correct:
            correct = f_correct.read().splitlines()
        with open(input.predicted) as f_predicted:
            predicted = f_predicted.read().splitlines()

        cer_value = cer_metric.compute(predictions=predicted, references=correct)
        chrf_value = chrf_metric.compute(
            predictions=predicted,
            references=[[x] for x in correct])

        with open(output[0], "w") as f_out:
            print(f"{cer_value},{chrf_value['score']}", file=f_out)
